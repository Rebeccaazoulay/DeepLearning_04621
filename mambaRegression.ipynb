{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7941127,"sourceType":"datasetVersion","datasetId":4668889}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mamba-ssm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-30T11:05:58.938138Z","iopub.execute_input":"2024-03-30T11:05:58.938830Z","iopub.status.idle":"2024-03-30T11:06:11.408436Z","shell.execute_reply.started":"2024-03-30T11:05:58.938797Z","shell.execute_reply":"2024-03-30T11:06:11.407300Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Requirement already satisfied: mamba-ssm in /opt/conda/lib/python3.10/site-packages (1.2.0.post1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (2.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (1.11.1.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (0.7.0)\nRequirement already satisfied: triton in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (2.2.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (4.38.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mamba-ssm) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (2024.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->mamba-ssm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->mamba-ssm) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport random\nimport statistics\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom statsmodels.discrete.discrete_model import Logit\nfrom statsmodels.tools import add_constant\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom mamba_ssm import Mamba","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:06:11.410902Z","iopub.execute_input":"2024-03-30T11:06:11.411702Z","iopub.status.idle":"2024-03-30T11:06:11.419911Z","shell.execute_reply.started":"2024-03-30T11:06:11.411661Z","shell.execute_reply":"2024-03-30T11:06:11.418983Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"### Define Global Variables ###\nglobal object_cols\nobject_cols = ['artist_name', 'track_id', 'track_name', 'key_notes', 'pop_cat']\n\nglobal numeric_cols\nnumeric_cols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness',\n                'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence',\n                'popularity', 'pop_frac', 'pop_bin']\n\nglobal categorical_cols\ncategorical_cols = ['key', 'mode', 'time_signature']\n\nglobal numeric_non_cat\nnumeric_non_cat = ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'liveness',\n                   'loudness', 'speechiness', 'tempo', 'valence',\n                   'popularity', 'pop_frac', 'pop_bin']\n\nglobal cols_to_stardardize\ncols_to_standardize = ['duration_ms', 'loudness', 'tempo']","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:06:11.421327Z","iopub.execute_input":"2024-03-30T11:06:11.421774Z","iopub.status.idle":"2024-03-30T11:06:11.438610Z","shell.execute_reply.started":"2024-03-30T11:06:11.421741Z","shell.execute_reply":"2024-03-30T11:06:11.437793Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"### ALL FUNCTIONS DEFINED HERE ####\n# load in main database of songs and attributes\ndef load_data():\n    df = pd.read_csv(Dataset_path)\n    return df\n\n# set some display options so easier to view all columns at once\ndef set_view_options(max_cols=50, max_rows=50, max_colwidth=9, dis_width=250):\n    pd.options.display.max_columns = max_cols\n    pd.options.display.max_rows = max_rows\n    pd.set_option('max_colwidth', max_colwidth)\n    pd.options.display.width = dis_width\n\n# allows for easier visualization of all columns at once in the terminal\ndef rename_columns(df):\n    df.columns = ['artist', 'trk_id', 'trk_name', 'acous', 'dance', 'ms',\n                  'energy', 'instr', 'key', 'live', 'loud', 'mode', 'speech',\n                  'tempo', 't_sig', 'val', 'popularity']\n    return df\n\ndef get_df_info(df):\n    # take an initial look at our data\n    print(df.head())\n\n    # take a look at the columns in our data set\n    print(\"The columns are:\")\n    print(df.columns)\n\n    # look at data types for each\n    print(df.info())\n\n    # take a look at data types, and it looks like we have a pretty clean data set!\n    # However, I think the 0 popularity scores might throw the model(s) off a bit.\n    print(\"Do we have any nulls?\")\n    print(f\"Looks like we have {df.isnull().sum().sum()} nulls\")\n\n    # Lets take a look at the average popularity score\n    pop_mean = df['popularity'].mean()\n    print(pop_mean)\n\n    # Proportion of songs that are very popular\n    print(df[df['popularity'] >= 50 ]['popularity'].count() / df.shape[0])\n\n    # Unique artists and song counts by artist\n    print(df['artist_name'].unique().shape)\n    print(df['artist_name'].value_counts())\n\n# nice way to truncate the column names to display easier\n# can be used with various metrics\ndef describe_cols(df, L=10):\n    '''Limit ENTIRE column width (including header)'''\n    # get the max col width\n    O = pd.get_option(\"display.max_colwidth\")\n    # set max col width to be L\n    pd.set_option(\"display.max_colwidth\", L)\n    print(df.rename(columns=lambda x: x[:L - 2] + '...' if len(x) > L else x).describe())\n    pd.set_option(\"display.max_colwidth\", O)\n\n# How many songs have a popularity score > 90??\n# Let's list these songs\ndef most_popular_songs(df):\n    most_popular = df[df['popularity'] > 90]['popularity'].count()\n    print(df[df['popularity'] > 90][['artist_name', 'popularity']])\n\n# plot a scatter plot\ndef scatter_plot(df, col_x, col_y):\n    plt.scatter(df[col_x], df[col_y], alpha=0.2)\n    plt.show()\n\ndef plot_scatter_matrix(df, num_rows):\n    scatter_matrix(df[:num_rows], alpha=0.2, figsize=(6, 6), diagonal='kde')\n    plt.show()\n\ndef calc_correlations(df, cutoff):\n    corr = df.corr()\n    print(corr[corr > cutoff])\n\n# get redundant pairs from DataFrame\ndef get_redundant_pairs(df):\n    '''Get diagonal pairs of correlation matrix and all pairs we'll remove\n    (since pair each is doubled in corr matrix)'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            if df[cols[i]].dtype != 'object' and df[cols[j]].dtype != 'object':\n                # print(\"THIS IS NOT AN OBJECT, YO, so you CAN take a corr of it, smarty!\")\n                pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\n# get top absolute correlations\ndef get_top_abs_correlations(df, n=10):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n\n    print(\"The top absolute correlations are:\")\n    print(au_corr[0:n])\n    return au_corr[0:n]\n\n# initial linear regression function, and plots\ndef linear_regression_initial(df):\n    df = df.copy()\n\n    X_cols = ['acousticness', 'danceability', 'duration_ms', 'energy',\n          'instrumentalness', 'key', 'liveness', 'loudness', 'mode',\n          'speechiness', 'tempo', 'time_signature', 'valence']\n\n    y_col = ['popularity']\n\n    X = df[X_cols]\n    y = df[y_col]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n    X_train = sm.add_constant(X_train)\n\n    # Instantiate OLS model, fit, predict, get errors\n    model = sm.OLS(y_train, X_train)\n    results = model.fit()\n    fitted_vals = results.predict(X_train)\n    stu_resid = results.resid_pearson\n    residuals = results.resid\n    y_vals = pd.DataFrame({'residuals':residuals, 'fitted_vals':fitted_vals, \\\n                           'stu_resid': stu_resid})\n\n    # Print the results\n    print(results.summary())\n\n    # QQ Plot\n    fig, ax = plt.subplots(figsize=(8, 5))\n    plt.title(\"QQ Plot - Initial Linear Regression\")\n    fig = sm.qqplot(stu_resid, line='45', fit=True, ax=ax)\n    plt.show()\n\n    # Residuals Plot\n    y_vals.plot(kind='scatter', x='fitted_vals', y='stu_resid')\n    plt.show()\n\n# print count of all zeros within the dataset\ndef get_zeros(df):\n    print(df[df['popularity'] == 0 ]['popularity'].count())\n\n# plot polularity scores distribution\ndef plot_pop_dist(df):\n    # set palette\n    sns.set_palette('muted')\n\n    # create initial figure\n    fig = plt.figure(figsize=(8,5))\n    ax = fig.add_subplot(111)\n    sns.distplot(df['popularity']/100, color='g', label=\"Popularity\").set_title(\"Distribution of Popularity Scores - Entire Data Set\")\n\n    # create x and y axis labels\n    plt.xlabel(\"Popularity\")\n    plt.ylabel(\"Density\")\n\n    plt.show()\n\n# plot undersampling methodology\ndef undersample_plot(df):\n    # set palette\n    sns.set_palette('muted')\n\n    # create initial figure\n    fig = plt.figure(figsize=(8,5))\n    ax = fig.add_subplot(111)\n    sns.distplot(df['popularity']/100, color='g', label=\"Popularity\").set_title(\"Illustration of Undersampling from Data Set\")\n\n    # create line to shade to the right of\n    line = ax.get_lines()[-1]\n    x_line, y_line = line.get_data()\n    mask = x_line > 0.55\n    x_line, y_line = x_line[mask], y_line[mask]\n    ax.fill_between(x_line, y1=y_line, alpha=0.5, facecolor='red')\n\n    # get values for and plot first label\n    label_x = 0.5\n    label_y = 4\n    arrow_x = 0.6\n    arrow_y = 0.2\n\n    arrow_properties = dict(\n        facecolor=\"black\", width=2,\n        headwidth=4,connectionstyle='arc3,rad=0')\n\n    plt.annotate(\n        \"First, sample all songs in this range.\\n Sample size is n. Cutoff is 0.5.\", xy=(arrow_x, arrow_y),\n        xytext=(label_x, label_y),\n        bbox=dict(boxstyle='round,pad=0.5', fc='red', alpha=0.5),\n        arrowprops=arrow_properties)\n\n    # Get values for and plot second label\n    label_x = 0.1\n    label_y = 3\n    arrow_x = 0.2\n    arrow_y = 0.2\n\n    arrow_properties = dict(\n        facecolor=\"black\", width=2,\n        headwidth=4,connectionstyle='arc3,rad=0')\n\n    plt.annotate(\n        \"Next, randomly sample \\n n songs in this range\", xy=(arrow_x, arrow_y),\n        xytext=(label_x, label_y),\n        bbox=dict(boxstyle='round,pad=0.5', fc='g', alpha=0.5),\n        arrowprops=arrow_properties)\n\n    # plot final word box\n    plt.annotate(\n        \"Therefore, end up with a 50/50 \\n split of Popular / Not Popular\\n songs\", xy=(0.6, 2),\n        xytext=(0.62, 2),\n        bbox=dict(boxstyle='round,pad=0.5', fc='b', alpha=0.5))\n\n    # create x and y axis labels\n    plt.xlabel(\"Popularity\")\n    plt.ylabel(\"Density\")\n\n    plt.show()\n\n# calculate and print more stats from the df\ndef get_stats(df):\n    # print stats for various metrics\n    print(f\"There are {df.shape[0]} rows\")\n    print(f\"There are {df['track_id'].unique().shape} unique songs\")\n    print(f\"There are {df['artist_name'].unique().shape} unique artists\")\n    print(f\"There are {df['popularity'].unique().shape} popularity scores\")\n    print(f\"The mean popularity score is {df['popularity'].mean()}\")\n    print(f\"There are {df[df['popularity'] > 55]['popularity'].count()} songs with a popularity score > 55\")\n    print(f\"There are {df[df['popularity'] > 75]['popularity'].count()} songs with a popularity score > 75\")\n    print(f\"Only {(df[df['popularity'] > 80]['popularity'].count() / df.shape[0])*100:.2f} % of songs have a popularity score > 80\")\n\n# plot univariate dists for several independent variables\ndef plot_univ_dists(df, cutoff):\n    popularity_cutoff = cutoff\n    print('Mean value for Danceability feature for Popular songs: {}'.format(df[df['popularity'] > popularity_cutoff]['danceability'].mean()))\n    print('Mean value for Danceability feature for Unpopular songs: {}'.format(df[df['popularity'] < popularity_cutoff]['danceability'].mean()))\n\n    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n    fig.suptitle('Histograms and Univariate Distributions of Important Features')\n    sns.distplot(df[df['popularity'] < popularity_cutoff]['danceability'])\n    sns.distplot(df[df['popularity'] > popularity_cutoff]['danceability'])\n    plt.show()\n\n    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n    sns.distplot(df[df['popularity'] < popularity_cutoff]['valence'])\n    sns.distplot(df[df['popularity'] > popularity_cutoff]['valence'])\n    plt.show()\n\n    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n    sns.distplot(df[df['popularity'] < popularity_cutoff]['acousticness'])\n    sns.distplot(df[df['popularity'] > popularity_cutoff]['acousticness'])\n    plt.show()\n\n# plot violin plot for several independent variables\ndef plot_violin(df, cutoff):\n    df = df.copy()\n\n    sns.set(style=\"whitegrid\")\n    df['pop_bin'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n\n    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12,4))\n    fig.suptitle('Distributions of Selected Features at Popularity Score Cutoff of 55')\n\n    sns.violinplot(x=df['pop_bin'], y=df['danceability'], ax=ax[0])\n    sns.violinplot(x=df['pop_bin'], y=df['valence'], ax=ax[1])\n    sns.violinplot(x=df['pop_bin'], y=df['acousticness'], ax=ax[2])\n\n    plt.show()\n\n    sns.set(style=\"whitegrid\")\n\n    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12,4))\n    fig.suptitle('Distributions of Selected Features at Popularity Score Cutoff of 55')\n\n    sns.violinplot(x=df['pop_bin'], y=df['energy'], ax=ax[0])\n    sns.violinplot(x=df['pop_bin'], y=df['instrumentalness'], ax=ax[1])\n    sns.violinplot(x=df['pop_bin'], y=df['liveness'], ax=ax[2])\n\n    plt.show()\n\n# plot pairplot for subsection of df rows and columns\ndef plot_pairplot(df, rows, cutoff):\n    # not it looks MUCH better to run this function in jupyter\n    df = df.copy()\n\n    df['pop_bin'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n\n    cols_for_pp = ['danceability', 'energy', 'instrumentalness',\n       'loudness','valence', 'popularity', 'pop_bin']\n\n    sns.pairplot(df.loc[:rows, cols_for_pp], hue='pop_bin', size=2)\n\n    plt.show()\n\n# plot the key counts for popular and unpopular songs\ndef plot_keys(df, cutoff):\n    df_popular = df[df['popularity'] > cutoff].copy()\n\n    fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,5))\n    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭', 4.0: 'E', 5.0:\n                  'F', 6.0: 'F♯,G♭', 7.0: 'G', 8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭',\n                  11.0: 'B'}\n\n    df_popular['key_val'] = df_popular['key'].map(key_mapping)\n    sns.countplot(x='key_val', data=df_popular, order=df_popular['key_val'].value_counts().index, palette='muted')\n    plt.title(\"Key Totals for Popular Songs\")\n    plt.show()\n\n    df_unpopular = df[df['popularity'] < 55].copy()\n    fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,5))\n    df_unpopular['key_val'] = df_unpopular['key'].map(key_mapping)\n    sns.countplot(x='key_val', data=df_unpopular, order=df_unpopular['key_val'].value_counts().index, palette='muted')\n    plt.title(\"Key Totals for Unpopular Songs\")\n    plt.show()\n\n# plot a heatmap of the correlations between features as well as dependent variable\ndef plot_heatmap(df):\n    # note this looks better in jupyter as well\n    plt.figure(figsize = (16,6))\n    sns.heatmap(df.corr(), cmap=\"coolwarm\", annot=True, )\n    plt.show()\n\n# check that deltas in means are significant for selected dependent variables\ndef calc_ANOVA(df, cutoff):\n    df_popular = df[df['popularity'] > cutoff].copy()\n    df_unpopular = df[df['popularity'] < cutoff].copy()\n\n    print(\"Popular and Unpopular Danceability Means:\")\n    print(df_popular['danceability'].mean())\n    print(df_unpopular['danceability'].mean())\n    f_val, p_val = stats.f_oneway(df_popular['danceability'], df_unpopular['danceability'])\n\n    print(\"Danceability One-way ANOVA P ={}\".format(p_val))\n\n    print(\"Popular and Unpopular Loudness Means:\")\n    print(df_popular['loudness'].mean())\n    print(df_unpopular['loudness'].mean())\n    f_val, p_val = stats.f_oneway(df_popular['loudness'], df_unpopular['loudness'])\n\n    print(\"Loudness One-way ANOVA P ={}\".format(p_val))\n\n    print(df_popular['valence'].mean())\n    print(df_unpopular['valence'].mean())\n    f_val, p_val = stats.f_oneway(df_popular['valence'], df_unpopular['valence'])\n\n    print(\"Valence One-way ANOVA P ={}\".format(p_val))\n\n    print(df_popular['instrumentalness'].mean())\n    print(df_unpopular['instrumentalness'].mean())\n    f_val, p_val = stats.f_oneway(df_popular['instrumentalness'], df_unpopular['instrumentalness'])\n\n    print(\"Instrumentalness One-way ANOVA P ={}\".format(p_val))\n\n# randomly sample data below cutoff after choosing a cutoff so have a 50/50 split\n# of popular/unpopular target variable values.\ndef random_under_sampler(df, cutoff):\n    df_original = df.copy()\n    df_original['pop_bin'] = np.where(df_original['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n\n    df_small = df_original[df_original['popularity'] > cutoff].copy()\n    df_samples_added = df_small.copy()\n\n    total = df_small.shape[0] + 1\n\n    # loop through and add random unpopular rows to sampled df\n    while total <= df_small.shape[0]*2:\n\n        # pick a random from from the original dataframe\n        rand_row = random.randint(0,df_original.shape[0])\n\n        if df_original.loc[rand_row, 'pop_bin'] == \"Not_Popular\":\n            df_samples_added.loc[total] = df_original.loc[rand_row, :]\n            total +=1\n\n    # print some stats on the undersampled df\n    print(\"Size checks for new df:\")\n    print(\"Shape of new undersampled df: {}\".format(df_samples_added.shape))\n    print(df_samples_added['pop_bin'].value_counts())\n    print(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'].mean())\n    print(df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'].mean())\n    print(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'].count())\n    print(df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'].count())\n    f_val, p_val = stats.f_oneway(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'], df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'])\n\n    print(\"One-way ANOVA P ={}\".format(p_val))\n\n    # return the df\n    return df_samples_added\n\n# plot histograms of metrics for popular and unpopular songs\ndef plot_hist(sampled_df):\n    sampled_df[sampled_df['pop_bin'] == \"Popular\"].hist(figsize=(8, 8))\n    plt.show()\n\n    sampled_df[sampled_df['pop_bin'] != \"Popular\"].hist(figsize=(8, 8))\n    plt.show()\n\n# return records that contain strings of artist and track names\ndef search_artist_track_name(df, artist, track):\n    # this displays much better in jupyter\n    print(df[(df['artist_name'].str.contains(artist)) & (df['track_name'].str.contains(track))])\n\n    # use this if searching for A$AP rocky (or other artist with $ in the name)\n    # df[(df['artist_name'].str.contains(\"A\\$AP Rocky\"))]\n\n# add important columns to dataframe\ndef add_cols(df, cutoff=55):\n    df = df.copy()\n\n    # add key_notes mapping key num vals to notes\n    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭',\n                   4.0: 'E', 5.0: 'F', 6.0: 'F♯,G♭', 7.0: 'G',\n                   8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭', 11.0: 'B'}\n    df['key_notes'] = df['key'].map(key_mapping)\n\n    # add columns relating to popularity\n    df['pop_frac'] = df['popularity'] / 100\n    df['pop_cat'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n    df['pop_bin'] = np.where(df['popularity'] > cutoff, 1, 0)\n\n    return df\n\ndef return_X_y_logistic_more_cols(df):\n    df = df.copy()\n\n    # define columns to use for each\n    X_cols = ['artist_name', 'track_id', 'track_name', 'acousticness', 'danceability', 'duration_ms', 'energy',\n              'instrumentalness', 'key', 'liveness', 'loudness', 'mode',\n              'speechiness', 'tempo', 'time_signature', 'valence']\n\n    # use 1's and 0's for logistic\n    y_col = ['pop_bin']\n\n    # split into X and y\n    X = df[X_cols]\n    y = df[y_col]\n\n    return X, y\n\ndef return_X_y_mamba_more_cols(df):\n    df = df.copy()\n\n    # define columns to use for each\n    X_cols = ['artist_name', 'track_id', 'track_name', 'acousticness', 'danceability', 'duration_ms', 'energy',\n              'instrumentalness', 'key', 'liveness', 'loudness', 'mode',\n              'speechiness', 'tempo', 'time_signature', 'valence']\n\n    y_col = ['popularity']\n\n    # split into X and y\n    X = df[X_cols]\n    y = df[y_col]\n\n    return X, y\n\n# choose cutoff, sample popular data, randomly sample unpopular data, and combine the dfs\ndef split_sample_combine(df, cutoff=55, col='popularity', rand=None):\n    # split out popular rows above the popularity cutoff\n    split_pop_df = df[df[col] > cutoff].copy()\n\n    # get the leftover rows, the 'unpopular' songs\n    df_leftover = df[df[col] < cutoff].copy()\n\n    # what % of the original data do we now have?\n    ratio = split_pop_df.shape[0] / df.shape[0]\n\n    # what % of leftover rows do we need?\n    ratio_leftover = split_pop_df.shape[0] / df_leftover.shape[0]\n\n    # get the exact # of unpopular rows needed, using a random sampler\n    unpop_df_leftover, unpop_df_to_add = train_test_split(df_leftover, \\\n                                                          test_size=ratio_leftover, \\\n                                                          random_state=rand)\n\n    # combine the dataframes to get total rows = split_pop_df * 2\n    # ssc stands for \"split_sample_combine\"\n    # Concatenate DataFrames using pandas.concat\n    ssc_df = pd.concat([split_pop_df, unpop_df_to_add], ignore_index=True)\n\n    # shuffle the df\n    ssc_df = ssc_df.sample(frac=1, random_state=rand).reset_index(drop=True)\n\n    # add key_notes mapping key num vals to notes\n    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭',\n                   4.0: 'E', 5.0: 'F', 6.0: 'F♯,G♭', 7.0: 'G',\n                   8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭', 11.0: 'B'}\n    ssc_df['key_notes'] = ssc_df['key'].map(key_mapping)\n\n    # add columns relating to popularity\n    ssc_df['pop_frac'] = ssc_df['popularity'] / 100\n    ssc_df['pop_cat'] = np.where(ssc_df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n    ssc_df['pop_bin'] = np.where(ssc_df['popularity'] > cutoff, 1, 0)\n\n    return ssc_df\n\ndef standardize_X(X):\n    X = X.copy()\n\n    # standardize only columns not between 0 and 1\n    for col in cols_to_standardize:\n        new_col_name = col + \"_std\"\n        X[new_col_name] = (X[col] - X[col].mean()) / X[col].std()\n\n    X_cols = ['acousticness', 'danceability', 'duration_ms_std', 'energy',\n              'instrumentalness', 'key', 'liveness', 'loudness_std', 'mode',\n              'speechiness', 'tempo_std', 'time_signature', 'valence']\n\n    # return the std columns in a dataframe\n    X = X[X_cols]\n\n    return X\n\ndef evaluation_metric(y_test,y_hat):\n    MSE = mean_squared_error(y_test, y_hat)\n    RMSE = MSE**0.5\n    MAE = mean_absolute_error(y_test,y_hat)\n    R2 = r2_score(y_test,y_hat)\n    print('MSE: %.4f, RMSE: %.4f, MAE: %.4f, R2: %.4f' % (MSE, RMSE, MAE, R2))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:06:11.440990Z","iopub.execute_input":"2024-03-30T11:06:11.441343Z","iopub.status.idle":"2024-03-30T11:06:11.526895Z","shell.execute_reply.started":"2024-03-30T11:06:11.441307Z","shell.execute_reply":"2024-03-30T11:06:11.525855Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def logistic_regression_final(df, plot_the_roc=True):\n    df = df.copy()\n    cutoff = 80\n\n    X, y = return_X_y_logistic_more_cols(split_sample_combine(df, cutoff=cutoff, rand=2))\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)\n\n    global df_train_results_log80\n    global df_test_results_log80\n    df_train_results_log80 = X_train.join(y_train)\n    df_test_results_log80 = X_test.join(y_test)\n\n    # standardize X_train and X_test\n    X_train = standardize_X(X_train)\n    X_test = standardize_X(X_test)\n\n    X_train = X_train.values\n    y_train = y_train.values.ravel()\n\n    X_test = X_test.values\n    y_test = y_test.values.ravel()\n\n    global sanity_check\n    sanity_check = X_test\n\n    ## Run logistic regression on all the data\n    classifier = LogisticRegression(max_iter = 1000)\n    # note using .predict_proba() below, which is the probability of each class\n\n    # predict values for X_train\n    y_predict_train = classifier.fit(X_train, y_train).predict(X_train)\n    probs_0and1_train = classifier.fit(X_train, y_train).predict_proba(X_train)\n    y_prob_P_train = probs_0and1_train[:, 1]\n\n    # predict values for X_test\n    y_predict_test = classifier.fit(X_train, y_train).predict(X_test)\n    probs_0and1_test = classifier.fit(X_train, y_train).predict_proba(X_test)  # yes!\n    y_prob_P_test = probs_0and1_test[:, 1]\n\n    # calculate metrics needed to use for ROC curve below\n    fpr_train, tpr_train, thresholds_train = metrics.roc_curve(y_train, y_prob_P_train, pos_label=1)\n    auc_train = metrics.roc_auc_score(y_train, y_prob_P_train)  # note we are scoring on our training data!\n\n    fpr_test, tpr_test, thresholds_test = metrics.roc_curve(y_test, y_prob_P_test, pos_label=1)\n    auc_test = metrics.roc_auc_score(y_test, y_prob_P_test)  # note we are scoring on our training data!\n\n    # print some metrics\n    print(\"Train accuracy: {:.2f}\".format(accuracy_score(y_train, y_predict_train)))\n    print(\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_predict_test)))\n\n    print(\"Train recall: {:.2f}\".format(recall_score(y_train, y_predict_train)))\n    print(\"Test recall: {:.2f}\".format(recall_score(y_test, y_predict_test)))\n\n    print(\"Train precision: {:.2f}\".format(precision_score(y_train, y_predict_train)))\n    print(\"Test precision: {:.2f}\".format(precision_score(y_test, y_predict_test)))\n\n    print(\"Train auc: {:.2f}\".format(auc_train))\n    print(\"Test auc: {:.2f}\".format(auc_test))\n\n    global conf_matrix_log80_train\n    global conf_matrix_log80_test\n    conf_matrix_log80_train = confusion_matrix(y_train, y_predict_train)\n    conf_matrix_log80_test = confusion_matrix(y_test, y_predict_test)\n\n    global final_coefs\n    global final_intercept\n    final_coefs = classifier.fit(X_train, y_train).coef_\n    final_intercept = classifier.fit(X_train, y_train).intercept_\n\n    # Back of the envelope calcs to make sure metrics above are correct\n    df_train_results_log80 = df_train_results_log80.reset_index(drop=True)\n    df_train_results_log80['pop_predict'] = y_prob_P_train\n\n    df_test_results_log80 = df_test_results_log80.reset_index(drop=True)\n    df_test_results_log80['pop_predict'] = y_prob_P_test\n\n    df_train_results_log80['pop_predict_bin'] = np.where(df_train_results_log80['pop_predict'] >= 0.5, 1, 0)\n    df_test_results_log80['pop_predict_bin'] = np.where(df_test_results_log80['pop_predict'] >= 0.5, 1, 0)\n\n    #print(\"Back of the envelope calc for Train Recall\")\n    #print(sum((df_train_results_log80['pop_predict_bin'].values * df_train_results_log80['pop_bin'].values)) /\n          #df_train_results_log80['pop_bin'].sum())\n\n    if plot_the_roc == True:\n        # Plot the ROC\n        fig = plt.figure(figsize=(10, 8))\n        ax = fig.add_subplot(111)\n        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n                label='Luck')\n        ax.plot(fpr_train, tpr_train, color='b', lw=2, label='Model_Train')\n        ax.plot(fpr_test, tpr_test, color='r', lw=2, label='Model_Test')\n        ax.set_xlabel(\"False Positive Rate\", fontsize=20)\n        ax.set_ylabel(\"True Positive Rate\", fontsize=20)\n        ax.set_title(\"ROC curve - Cutoff: \" + str(cutoff), fontsize=24)\n        ax.text(0.05, 0.95, \" \".join([\"AUC_train:\", str(auc_train.round(3))]), fontsize=20)\n        ax.text(0.32, 0.7, \" \".join([\"AUC_test:\", str(auc_test.round(3))]), fontsize=20)\n        ax.legend(fontsize=24)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:06:11.528206Z","iopub.execute_input":"2024-03-30T11:06:11.528497Z","iopub.status.idle":"2024-03-30T11:06:11.549207Z","shell.execute_reply.started":"2024-03-30T11:06:11.528473Z","shell.execute_reply":"2024-03-30T11:06:11.548019Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(df):\n    # Convert non-numeric columns to numeric types if possible\n    for column in df.columns:\n        if pd.api.types.is_numeric_dtype(df[column]):\n            continue\n        try:\n            df[column] = pd.to_numeric(df[column])\n        except ValueError:\n            df.drop(column, axis=1, inplace=True)  # Drop non-numeric column if conversion fails\n    return df\n\nclass MambaModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(MambaModel, self).__init__()\n        self.mamba = Mamba(\n            d_model=input_dim,\n            d_state=16,\n            d_conv=4,\n            expand=2\n        )\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        x = self.mamba(x)\n        x = self.linear(x)\n        x = torch.sigmoid(x)\n        return x.flatten()\n\ndef PredictWithData(X_train, y_train, X_test):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")                \n    learning_rate = 0.01\n    num_epochs = 1000\n    \n    model = MambaModel(input_dim = X_train.shape[1], output_dim = 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n    criterion = nn.MSELoss() # For Regression\n    \n    X_tr = torch.from_numpy(X_train).float().unsqueeze(0).to(device)\n    X_te = torch.from_numpy(X_test).float().unsqueeze(0).to(device)\n    y_tr = torch.from_numpy(y_train).float().to(device)\n\n    for e in range(1, num_epochs+1):\n        model.train()\n        permutation = torch.randperm(X_tr.size()[1])  # Data shuffling\n        X_tr = X_tr[:,permutation,:]\n        y_tr = y_tr[permutation]\n        z = 100*model(X_tr) # For Regression\n        loss = criterion(z, y_tr)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if e%100 == 0:\n            print('Epoch %d | Lossp: %.4f' % (e, loss.item()))\n            \n    model.eval()\n    mat = model(X_te)\n    mat = mat.cpu()\n    yhat = mat.detach().numpy().flatten()\n    yhat = np.round(100*yhat)\n    yhat = yhat.astype(int)\n    return yhat\n\ndef ourModel(df):\n    # Calculate the cutoff = median of popularity\n    df1 = df.copy()\n    df1 = preprocess_data(df1)\n    labels = df1['popularity']\n    cutoff = statistics.median(labels)\n    print(\"cutoff:\", cutoff)\n    \n    # Process and split the dataset\n    df = df.copy()\n    X, y = return_X_y_mamba_more_cols(split_sample_combine(df, cutoff=cutoff, rand=2))\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)\n    \n    # Standardize X_train and X_test\n    X_train = standardize_X(X_train)\n    X_test = standardize_X(X_test)\n\n    X_train = X_train.values\n    y_train = y_train.values.ravel()\n\n    X_test = X_test.values\n    y_test = y_test.values.ravel()\n\n    # Run inference\n    y_pred = PredictWithData(X_train, y_train, X_test)\n    \n    # Convert predictions to binary\n    y_pred_bin = (y_pred >= cutoff).astype(int)\n    y_test_bin = (y_test >= cutoff).astype(int)\n    \n    # Compute metrics\n    accuracy = accuracy_score(y_test_bin, y_pred_bin)\n    recall = recall_score(y_test_bin, y_pred_bin)\n    precision = precision_score(y_test_bin, y_pred_bin)\n    \n    # Print metrics\n    print(\"Accuracy: {:.2f}\".format(accuracy))\n    print(\"Recall: {:.2f}\".format(recall))\n    print(\"Precision: {:.2f}\".format(precision))\n    \n    return y_pred_bin","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:06:11.550876Z","iopub.execute_input":"2024-03-30T11:06:11.551199Z","iopub.status.idle":"2024-03-30T11:06:11.571431Z","shell.execute_reply.started":"2024-03-30T11:06:11.551161Z","shell.execute_reply":"2024-03-30T11:06:11.570510Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load data\n    Dataset_path = '/kaggle/input/spotifydataset/SpotifyDataset.csv'\n    df = load_data()\n\n    ourModel(df)\n    #logistic_regression_final(df, plot_the_roc=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:06:11.572739Z","iopub.execute_input":"2024-03-30T11:06:11.573369Z","iopub.status.idle":"2024-03-30T11:06:14.571799Z","shell.execute_reply.started":"2024-03-30T11:06:11.573337Z","shell.execute_reply":"2024-03-30T11:06:14.570737Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"cutoff: 67\nEpoch 100 | Lossp: 113.5432\nEpoch 200 | Lossp: 109.2435\nEpoch 300 | Lossp: 104.5987\nEpoch 400 | Lossp: 102.5124\nEpoch 500 | Lossp: 100.3509\nEpoch 600 | Lossp: 96.8474\nEpoch 700 | Lossp: 94.2266\nEpoch 800 | Lossp: 90.9605\nEpoch 900 | Lossp: 89.8394\nEpoch 1000 | Lossp: 87.8450\nAccuracy: 0.63\nRecall: 0.62\nPrecision: 0.65\n","output_type":"stream"}]}]}